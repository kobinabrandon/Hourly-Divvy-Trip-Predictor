{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T11:04:43.441673192Z",
     "start_time": "2023-12-25T11:04:42.693396090Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.core.paths import ALTERED_DATA_TYPES, TEMPORARY_DATA, CLEANED_DATA\n",
    "from src.core.miscellaneous import find_first_nan, use_primary_geocoder, use_secondary_geocoder, add_coordinates_to_dataframe, find_rows_with_zeros, reveal_final_unknown_lats, concat_to_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2014 - 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2014 = pd.read_parquet(ALTERED_DATA_TYPES/\"2014 - 2017/0.parquet\")\n",
    "trips_2015 = pd.read_parquet(ALTERED_DATA_TYPES/\"2014 - 2017/1.parquet\")\n",
    "trips_2016 = pd.read_parquet(ALTERED_DATA_TYPES/\"2014 - 2017/2.parquet\")\n",
    "trips_2017 = pd.read_parquet(ALTERED_DATA_TYPES/\"2014 - 2017/3.parquet\")\n",
    "\n",
    "pre_2018 = [trips_2014, trips_2015, trips_2016, trips_2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Dropping Irrelevant Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2014 : 'starttime', 'stoptime'\n",
    "2015 : 'starttime', 'stoptime'\n",
    "2016 : 'starttime', 'stoptime'\n",
    "2017 : 'start_time', 'end_time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### All four of these dataframes have the same column names. I am removing the station IDs (for the station at the beginning of each trip) because I will be more interested in deriving the latitudes and longitudes of each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing said columns\n",
    "for dataset in pre_2018:\n",
    "\n",
    "    dataset.drop(\n",
    "        columns = [\"trip_id\", \"bikeid\", \"tripduration\", \"from_station_id\", \"to_station_id\", \"birthyear\", \"gender\"],\n",
    "        inplace = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Renaming certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset in pre_2018[:3]:\n",
    "\n",
    "    # Rename some of the columns\n",
    "    dataset.rename(\n",
    "        columns = {\n",
    "            \"usertype\" : \"user_type\",\n",
    "            \"starttime\" : \"start_time\",\n",
    "            \"stoptime\" : \"stop_time\"\n",
    "        }, inplace = True\n",
    "    )\n",
    "    \n",
    "trips_2017.rename(\n",
    "    columns = {\n",
    "        \"end_time\" : \"stop_time\",\n",
    "        \"usertype\" : \"user_type\"\n",
    "    }, inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset in pre_2018:\n",
    "\n",
    "    print(dataset.isna().sum())\n",
    "    print(\"######################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Checking for Duplicates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a list comprehension to view the number of duplicated observations in each dataset\n",
    "[\n",
    "    dataset[dataset.duplicated(keep = \"last\") == True].shape[0] for dataset in pre_2018\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### All four datasets from 2014 to 2017 contain duplicated observations, so we remove them in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset in pre_2018:\n",
    "\n",
    "    dataset.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_pre_2018 = concat_to_parquet(list_of_dataframes = pre_2018, folder_name = CLEANED_DATA, parquet_name = \"trips_pre_2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2018 - 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018 = pd.read_parquet(ALTERED_DATA_TYPES/\"2018 - 2019/0.parquet\")\n",
    "trips_2018.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.drop(\n",
    "    columns = [\n",
    "        \"Member Gender\", \"05 - Member Details Member Birthday Year\", \"trip_id\", \"from_station_id\", \"to_station_id\",\n",
    "        \"01 - Rental Details Duration In Seconds Uncapped\", \"tripduration\", \"birthyear\", \"gender\", \"bikeid\",\n",
    "        \"01 - Rental Details Rental ID\", \"03 - Rental Start Station ID\", \"02 - Rental End Station ID\",\n",
    "        \"01 - Rental Details Bike ID\"\n",
    "    ], inplace = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Viewing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Looking at the structure of 2018's dataframe, it would appear that this dataframe it is a diagonal matrix. It has 3,603,082 rows. Let us consider how many missing values there are in the \"lower right\" section of what appears to be a diagonal matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.iloc[:,6:].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### As stated, I suspect that this is a diagonal matrix. To confirm this, I have to investigate the number of missing values before and after the line that divides the data into two vertical sections. We see for instance that there are 387,145 missing values in each of the rows of this \"lower right\" section of the matrix.\n",
    "\n",
    "###### Let us check how many missing values there are on the \"upper left\" section of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.iloc[:,:6].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Notice that we would have a major piece of supporting evidence (in favour of my suspicion) if the sum of the number of missing values from the two sections equalled the total number of missing values in the whole matrix. This is exactly the case, since 3,215,937 + 387,145 = 3,603,082.\n",
    "\n",
    "###### We need to find out on which row (of the \"lower left\" dataframe) the missing values start, so that we can see where this empty block of data begins. It would be a mistake to assume that the missing values on the \"lower left\" start at the half-way point (row-wise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the first row where a missing value occurs\n",
    "trips_2018.apply(pd.Series.last_valid_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### The first missing value occurs on row #387,145. And for further confirmation, let us check whether there are any non-missing values after this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.iloc[387145:,:].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Just to be thorough, let's see when the missing values on the \"top left\" end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.apply(pd.Series.first_valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the first row where a missing value occurs\n",
    "right_half_2018 = [find_first_nan(data = trips_2018.iloc[:,i:], missing = False, just_reveal = False) for i in range(5,9)]\n",
    "right_half_2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### They end at the same place where they begin on the \"top right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Forming the Final 2018 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_left_2018 = trips_2018.iloc[:387145, :5]\n",
    "bottom_right_2018 = trips_2018.iloc[387145:, 5:]\n",
    "\n",
    "# Final Renaming of columns\n",
    "top_left_2018.rename(columns = {\n",
    "                    \"01 - Rental Details Local Start Time\": \"start_time\",\n",
    "                    \"01 - Rental Details Local End Time\": \"end_time\",\n",
    "                    \"03 - Rental Start Station Name\": \"from_station_name\",\n",
    "                    \"02 - Rental End Station Name\": \"to_station_name\",\n",
    "                    \"User Type\": \"user_type\"}, inplace = True)\n",
    "\n",
    "\n",
    "bottom_right_2018.rename(columns = {\"usertype\": \"user_type\"}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018 = pd.concat([top_left_2018, bottom_right_2018], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2018.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Viewing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019 = pd.read_parquet(ALTERED_DATA_TYPES/\"2018 - 2019/1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.reset_index(inplace = True)\n",
    "trips_2019.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Let us remove the columns that we are not going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.drop(\n",
    "    columns = [\n",
    "        \"trip_id\",  \"gender\", \"birthyear\", \"01 - Rental Details Rental ID\", \"from_station_id\",\n",
    "        \"01 - Rental Details Bike ID\", \"03 - Rental Start Station ID\", \"02 - Rental End Station ID\",\n",
    "        \"Member Gender\", \"bikeid\", \"05 - Member Details Member Birthday Year\", \"to_station_id\",\n",
    "        \"index\", \"01 - Rental Details Duration In Seconds Uncapped\", \"tripduration\"\n",
    "    ], inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Dealing With Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### The 2019 dataframe has 3,818,004 rows. Let us check for the number of missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### As with the 2018 data, there are (in reality) only 5 columns here, but those columns have been duplicated under different names. The number of rows has been divided in the following manner: in the first 5 columns, 1,108,163 of the values are missing values. The second set of 5 columns (which I consider to be a duplicate of the first set) there are 2,709,841 missing values.\n",
    "\n",
    "###### Let us have a look at the exact structure of these blocks of missing values is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.apply(pd.Series.first_valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produce a list consisting of the row index on which a NaN first occurs for each column\n",
    "first_nan_locations_2019 = [\n",
    "    find_first_nan(data = trips_2019.iloc[:,i:], missing = True, just_reveal = False) for i in range(0,10)\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View said list \n",
    "first_nan_locations_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### The missing values in the first set of 5 columns begin on row #356,069. Those in the second set of 5 columns start from the beginning. So let us invert the question by asking when the non-missing values in the data start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produce a list consisting of the row index on which a non-NaN first occurs for each column\n",
    "first_non_nan_locations_2019 = [\n",
    "    find_first_nan(data = trips_2019.iloc[:,i:], missing = False, just_reveal = False) for i in range(0,10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Again, view said list\n",
    "first_non_nan_locations_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### The non-missing values in the second set of 5 columns also begin on row #356,069. This suggests that the missing values in one set of 5 columns are possibly present in the other set. If true, this would mean that, as a result of the evident duplication of columns, there are in fact no missing values in the data.\n",
    "\n",
    "###### Let us check whether the number of missing values remains constant for every row. This would lend some credence to our budding theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in first_nan_locations_2019:\n",
    "\n",
    "    row_nan_count = 0\n",
    "    \n",
    "    for j in range(0,10):\n",
    "\n",
    "        if pd.isnull(trips_2019.iloc[i, j]):\n",
    "\n",
    "            row_nan_count += 1\n",
    "\n",
    "    print(f\"There are {row_nan_count} missing values on row #{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Every row that contains missing values contains exactly 5 of them. Let us go one step further to confirm the theory. I will begin to isolate the values in the left half of the dataset, and check whether they are all missing. We have already seen that there are 1,108,163 missing values in the left half of the data, and we have seen that the missing values start from row #365069 for its 5 columns. What we want to confirm now is whether or not these 1,108,163 missing values all occur exactly one after the other in a single unbroken sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Isolate the missing values in the left half of the dataset\n",
    "trips_2019.iloc[365069: 365069 + 1108163, :5].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Indeed they do. \n",
    "\n",
    "###### Let us investigate the missing values in the second half of the data. To begin with, we know that the first 365,069 values are missing. But we also know that there are 2,709,841 missing values in each column of the second half of the data. We know that 365,070 is not a missing value. But where are the remaining 2,344,772 values (per column)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.iloc[365069:, :10].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produce a list consisting of the row index on which a NaN first occurs for each column\n",
    "find_first_nan(data = trips_2019.iloc[365069:, 5:10], missing = True, just_reveal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_first_nan(data = trips_2019.iloc[1108163:, 5:10], missing = True, just_reveal = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### We have found the remaining 2,344,772 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.iloc[365069+1108163:, 5:10].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Below, we see that in the preceding five columns, these values are present in the same rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.iloc[365069+1108163:, :5].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Forming the Final 2019 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.rename(\n",
    "    columns = {\n",
    "        \"usertype\":\"user_type\",\n",
    "        \"end_time\":\"stop_time\"\n",
    "    }, inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_left_2019 = trips_2019.iloc[:365069,:5]\n",
    "bottom_left_2019 = trips_2019.iloc[365069+1108163:,:5]\n",
    "right_side_2019 = trips_2019.iloc[365069:365069+1108163,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right_side_2019.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### We need to rename the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right_side_2019.rename(\n",
    "    columns = {\"01 - Rental Details Local Start Time\": \"start_time\", \n",
    "               \"01 - Rental Details Local End Time\": \"stop_time\", \n",
    "               \"03 - Rental Start Station Name\": \"from_station_name\", \n",
    "               \"02 - Rental End Station Name\": \"to_station_name\",\n",
    "               \"User Type\": \"user_type\" \n",
    "               }, inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bottom_left_2019.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### We attach these components together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019 = pd.concat(\n",
    "    [\n",
    "        top_left_2019, bottom_left_2019, right_side_2019\n",
    "    ], axis = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### I almost forgot to remove duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2019.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### We need to make \"trip_duration\" a column of floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Geocoding the data from 2014 - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_2020 = [trips_2014, trips_2015, trips_2016, trips_2017, trips_2018, trips_2019]\n",
    "\n",
    "trips_pre_2020 = concat_to_parquet(list_of_dataframes = pre_2020, folder_name = CLEANED_DATA, parquet_name = \"pre_2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T11:03:48.314360199Z",
     "start_time": "2023-12-25T11:03:46.195920102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_pre_2020 = pd.read_parquet(CLEANED_DATA/\"pre_2020.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Points of Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### It's important to have the unique station names for geocoding, lest we geocode a bunch of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T11:03:51.873987277Z",
     "start_time": "2023-12-25T11:03:50.870633260Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origins = list(trips_pre_2020[\"from_station_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "######  In fact, if we specify that we don't want duplicates, we get 743 station names, instead of 20,463,107 entries.\n",
    "###### So we get the Nominatim geocoder to obtain the coordinates for each place, and associate each place with its precise location. Then we will need to put the latitudes and longitudes of each point in their respective columns of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T11:04:24.650539291Z",
     "start_time": "2023-12-25T11:03:51.965976168Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "places_and_points = use_primary_geocoder(places = origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-25T11:04:24.648678943Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_coordinates_to_dataframe(\n",
    "    data = trips_pre_2020,\n",
    "    places_and_points = places_and_points,\n",
    "    start_or_stop = \"start\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T11:04:24.665052434Z",
     "start_time": "2023-12-25T11:04:24.652338988Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "destinations = list(\n",
    "    trips_pre_2020[\"to_station_name\"].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### If there are some destinations which are not among the points of origin, then we will have to geocode them. Otherwise, there'll be no need for further geocoding. So let us check for the presence of any such destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-25T11:04:24.654742353Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_in_common = [origin for origin in origins if origin not in destinations]\n",
    "len(not_in_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### There are no such destinations, and consequently, the \"to_station_name\" column does not need to be geocoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_coordinates_to_dataframe(\n",
    "    data = trips_pre_2020,\n",
    "    places_and_points = places_and_points,\n",
    "    start_or_stop = \"stop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_pre_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-25T11:00:00.897722526Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_pre_2020.to_parquet(path = TEMPORARY_DATA/\"geocoded_pre_2020.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Final Clean of the data from 2014-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T11:07:43.079366909Z",
     "start_time": "2023-12-25T11:07:39.796124934Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_pre_2020 = pd.read_parquet(TEMPORARY_DATA/\"geocoded_pre_2020.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dealing with the rows which contain station names that could not be geocoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Finding all the rows (by indices) which contain data that could not be geocoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_pre_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_unknown_start_lats = find_rows_with_zeros(data = trips_pre_2020, column_index = 5)\n",
    "rows_with_unknown_start_lngs = find_rows_with_zeros(data = trips_pre_2020, column_index = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Find out how many such rows there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(rows_with_unknown_start_lats), len(rows_with_unknown_start_lngs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### We have the same number of rows for which the latitudes and longitudes of the points of origin were not obtained. This makes sense, since it is probably unlikely for the geocoder to work for one coordinate, but not the other. To confirm this, let us check whether the rows for which the latitudes and longitudes were not geocoded are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(\n",
    "    [row for row in rows_with_unknown_start_lats if row in rows_with_unknown_start_lngs]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_unknown_stop_lats = find_rows_with_zeros(data = trips_pre_2020, column_index = 7)\n",
    "rows_with_unknown_stop_lngs = find_rows_with_zeros(data = trips_pre_2020, column_index = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(rows_with_unknown_stop_lats), len(rows_with_unknown_stop_lngs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Again, we have the same number of rows for which the latitudes and longitudes of the station names were not obtained. To confirm this, let us check whether the rows for which the latitudes and longitudes were not geocoded are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(\n",
    "    [row for row in rows_with_unknown_start_lats if row in rows_with_unknown_start_lngs]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Using a secondary geocoder (Photon) to geocode the outstanding points of origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### The alternative would have been to delete all rows in which the origin and destination stations were unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:51.918841639Z",
     "start_time": "2023-12-23T08:14:51.743851100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_geocoded_origins = list(\n",
    "    trips_pre_2020.iloc[rows_with_unknown_start_lats, 2].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:52.124967964Z",
     "start_time": "2023-12-23T08:14:51.924538712Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remaining_origins_with_points = use_secondary_geocoder(\n",
    "    data = trips_pre_2020,\n",
    "    column_of_station_names = \"from_station_name\",\n",
    "    row_indices = rows_with_unknown_start_lats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Placing the obtained coordinates of the origins in the appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T08:14:52.009616213Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row, point in zip(\n",
    "        rows_with_unknown_start_lats, remaining_origins_with_points.values()\n",
    "):\n",
    "    \n",
    "    trips_pre_2020[\"start_latitude\"].replace(trips_pre_2020.iloc[row, 5], point[0])\n",
    "    trips_pre_2020[\"start_longitude\"].replace(trips_pre_2020.iloc[row, 6], point[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Saving the current state of the data to save time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Using the Photon geocoder to geocode the outstanding destinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:52.356885301Z",
     "start_time": "2023-12-23T08:14:52.127354448Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remaining_destinations_with_points = use_secondary_geocoder(\n",
    "    data = trips_pre_2020,\n",
    "    column_of_station_names = \"to_station_name\",\n",
    "    row_indices = rows_with_unknown_stop_lats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Placing the obtained coordinates of the destinations in the appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T08:14:52.241791890Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row, point in zip(\n",
    "        rows_with_unknown_stop_lats, remaining_destinations_with_points.values()\n",
    "):\n",
    "\n",
    "    trips_pre_2020[\"stop_latitude\"].replace(trips_pre_2020.iloc[row, 7], point[0])\n",
    "    trips_pre_2020[\"stop_longitude\"].replace(trips_pre_2020.iloc[row, 8], point[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Save the current state of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:52.684843123Z",
     "start_time": "2023-12-23T08:14:52.475874534Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_pre_2020.to_parquet(path = CLEANED_DATA/\"pre_2020.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Replace the last few latitudes and longitudes that were not geocoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T08:14:52.660713737Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_pre_2020 = pd.read_parquet(CLEANED_DATA/\"pre_2020.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:52.925052356Z",
     "start_time": "2023-12-23T08:14:52.763821329Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_unknown_origin_lats = reveal_final_unknown_lats(data = trips_pre_2020, column_of_coordinate = \"start_latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T08:14:52.897123660Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(final_unknown_origin_lats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Many of the latitudes are 41, and many of the longitudes are around -87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\"start_latitude\", \"start_longitude\"]:\n",
    "\n",
    "    trips_pre_2020[column].replace(0.0, 41, inplace = True)\n",
    "\n",
    "\n",
    "for column in [\"stop_latitude\", \"stop_longitude\"]:\n",
    "\n",
    "    trips_pre_2020[column].replace(0.0, -87, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_pre_2020.drop(columns = [\"from_station_name\", \"to_station_name\", \"user_type\"], inplace = True)\n",
    "trips_pre_2020.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_pre_2020 = trips_pre_2020.to_parquet(path = CLEANED_DATA/\"final_pre_2020.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 - 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_pre_2020 = pd.read_parquet(path = CLEANED_DATA/\"final_pre_2020.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T08:14:53.094333098Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_2020 = pd.read_parquet(ALTERED_DATA_TYPES/\"2020 - 2023/0.parquet\")\n",
    "trips_2021 = pd.read_parquet(ALTERED_DATA_TYPES/\"2020 - 2023/1.parquet\")\n",
    "trips_2022 = pd.read_parquet(ALTERED_DATA_TYPES/\"2020 - 2023/2.parquet\")\n",
    "trips_2023 = pd.read_parquet(ALTERED_DATA_TYPES/\"2020 - 2023/3.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Removing some unnecessary features, and renaming others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:53.449595341Z",
     "start_time": "2023-12-23T08:14:53.228368289Z"
    }
   },
   "outputs": [],
   "source": [
    "from_2020 = [trips_2020, trips_2021, trips_2022, trips_2023]\n",
    "\n",
    "for dataset in from_2020:\n",
    "\n",
    "    #  I no longer need the station names\n",
    "    dataset.drop(columns = [\"ride_id\", \"rideable_type\", \"member_casual\"], inplace = True)\n",
    "\n",
    "    dataset.rename(\n",
    "        columns = {\n",
    "            \"started_at\": \"start_time\", \n",
    "            \"ended_at\" : \"stop_time\",\n",
    "            \"start_station_name\" : \"from_station_name\",\n",
    "            \"end_station_name\" : \"to_station_name\", \n",
    "            \"start_lat\" : \"start_latitude\",\n",
    "            \"start_lng\" : \"start_longitude\",\n",
    "            \"end_lat\" : \"stop_latitude\",\n",
    "            \"end_lng\" : \"stop_longitude\",\n",
    "            },\n",
    "        inplace = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Checking out the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For each year, there are a couple of thousand trips for which the destination's coordinates and names are unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_from_2020 = pd.concat(from_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_time                 0\n",
       "stop_time                  0\n",
       "from_station_name    2458535\n",
       "to_station_name      2634071\n",
       "start_latitude             0\n",
       "start_longitude            0\n",
       "stop_latitude          21635\n",
       "stop_longitude         21635\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_from_2020.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let us see whether there are a siginificant number of instances where the destination names are present, without correponding coordinates. If so, geocoding will be justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>start_latitude</th>\n",
       "      <th>start_longitude</th>\n",
       "      <th>stop_latitude</th>\n",
       "      <th>stop_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414426</th>\n",
       "      <td>2020-03-16 11:23:36</td>\n",
       "      <td>2020-03-16 11:23:24</td>\n",
       "      <td>HQ QR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.889900</td>\n",
       "      <td>-87.680300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2020-04-07 11:53:08</td>\n",
       "      <td>2020-04-07 12:28:35</td>\n",
       "      <td>Wells St &amp; Concord Ln</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.912100</td>\n",
       "      <td>-87.634700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>2020-04-20 12:24:48</td>\n",
       "      <td>2020-04-20 12:29:46</td>\n",
       "      <td>Racine Ave &amp; Wrightwood Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.928900</td>\n",
       "      <td>-87.659000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>2020-04-16 08:41:56</td>\n",
       "      <td>2020-04-16 11:33:48</td>\n",
       "      <td>Racine Ave &amp; 18th St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.858200</td>\n",
       "      <td>-87.656500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>2020-04-09 15:33:45</td>\n",
       "      <td>2020-04-09 16:34:54</td>\n",
       "      <td>Morgan Ave &amp; 14th Pl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.862400</td>\n",
       "      <td>-87.651100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356624</th>\n",
       "      <td>2023-11-23 19:51:43</td>\n",
       "      <td>2023-11-24 20:51:39</td>\n",
       "      <td>Clark St &amp; Armitage Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.918306</td>\n",
       "      <td>-87.636282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356625</th>\n",
       "      <td>2023-11-23 19:49:48</td>\n",
       "      <td>2023-11-24 20:49:42</td>\n",
       "      <td>Clark St &amp; Armitage Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.918306</td>\n",
       "      <td>-87.636282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356661</th>\n",
       "      <td>2023-11-09 15:37:23</td>\n",
       "      <td>2023-11-10 16:37:19</td>\n",
       "      <td>Clark St &amp; Jarvis Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.015963</td>\n",
       "      <td>-87.675005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356677</th>\n",
       "      <td>2023-11-16 08:05:05</td>\n",
       "      <td>2023-11-17 09:04:58</td>\n",
       "      <td>Clark St &amp; Wellington Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.936497</td>\n",
       "      <td>-87.647539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356698</th>\n",
       "      <td>2023-11-19 03:46:08</td>\n",
       "      <td>2023-11-20 04:46:02</td>\n",
       "      <td>Rush St &amp; Hubbard St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.890173</td>\n",
       "      <td>-87.626185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21519 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time           stop_time            from_station_name  \\\n",
       "414426 2020-03-16 11:23:36 2020-03-16 11:23:24                        HQ QR   \n",
       "1001   2020-04-07 11:53:08 2020-04-07 12:28:35        Wells St & Concord Ln   \n",
       "1864   2020-04-20 12:24:48 2020-04-20 12:29:46  Racine Ave & Wrightwood Ave   \n",
       "2167   2020-04-16 08:41:56 2020-04-16 11:33:48         Racine Ave & 18th St   \n",
       "2458   2020-04-09 15:33:45 2020-04-09 16:34:54         Morgan Ave & 14th Pl   \n",
       "...                    ...                 ...                          ...   \n",
       "356624 2023-11-23 19:51:43 2023-11-24 20:51:39      Clark St & Armitage Ave   \n",
       "356625 2023-11-23 19:49:48 2023-11-24 20:49:42      Clark St & Armitage Ave   \n",
       "356661 2023-11-09 15:37:23 2023-11-10 16:37:19        Clark St & Jarvis Ave   \n",
       "356677 2023-11-16 08:05:05 2023-11-17 09:04:58    Clark St & Wellington Ave   \n",
       "356698 2023-11-19 03:46:08 2023-11-20 04:46:02         Rush St & Hubbard St   \n",
       "\n",
       "       to_station_name  start_latitude  start_longitude  stop_latitude  \\\n",
       "414426             NaN       41.889900       -87.680300            NaN   \n",
       "1001               NaN       41.912100       -87.634700            NaN   \n",
       "1864               NaN       41.928900       -87.659000            NaN   \n",
       "2167               NaN       41.858200       -87.656500            NaN   \n",
       "2458               NaN       41.862400       -87.651100            NaN   \n",
       "...                ...             ...              ...            ...   \n",
       "356624             NaN       41.918306       -87.636282            NaN   \n",
       "356625             NaN       41.918306       -87.636282            NaN   \n",
       "356661             NaN       42.015963       -87.675005            NaN   \n",
       "356677             NaN       41.936497       -87.647539            NaN   \n",
       "356698             NaN       41.890173       -87.626185            NaN   \n",
       "\n",
       "        stop_longitude  \n",
       "414426             NaN  \n",
       "1001               NaN  \n",
       "1864               NaN  \n",
       "2167               NaN  \n",
       "2458               NaN  \n",
       "...                ...  \n",
       "356624             NaN  \n",
       "356625             NaN  \n",
       "356661             NaN  \n",
       "356677             NaN  \n",
       "356698             NaN  \n",
       "\n",
       "[21519 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_from_2020[(pd.isnull(trips_from_2020[\"to_station_name\"])) & (pd.isnull(trips_from_2020[\"stop_latitude\"])) & \n",
    "(pd.isnull(trips_from_2020[\"stop_longitude\"])) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In almost all instances, both the destination names and the corresponding coordinates are missing. Therefore, it is justified that I replace the missing coordinates with likely values (as I did before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\"start_latitude\", \"start_longitude\"]:\n",
    "\n",
    "    trips_from_2020[column].fillna(value = 41, inplace = True)\n",
    "\n",
    "for column in [\"stop_latitude\", \"stop_longitude\"]:\n",
    "\n",
    "    trips_from_2020[column].fillna(value = -87, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### I don't need the names of the origins and destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:53.656914057Z",
     "start_time": "2023-12-23T08:14:53.492618760Z"
    }
   },
   "outputs": [],
   "source": [
    "trips_from_2020.drop(\n",
    "        columns = [\"from_station_name\", \"to_station_name\"],\n",
    "        inplace = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>start_latitude</th>\n",
       "      <th>start_longitude</th>\n",
       "      <th>stop_latitude</th>\n",
       "      <th>stop_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21 20:06:59</td>\n",
       "      <td>2020-01-21 20:14:30</td>\n",
       "      <td>41.966500</td>\n",
       "      <td>-87.688400</td>\n",
       "      <td>41.967100</td>\n",
       "      <td>-87.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-30 14:22:39</td>\n",
       "      <td>2020-01-30 14:26:22</td>\n",
       "      <td>41.961600</td>\n",
       "      <td>-87.666000</td>\n",
       "      <td>41.954200</td>\n",
       "      <td>-87.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-09 19:29:26</td>\n",
       "      <td>2020-01-09 19:32:17</td>\n",
       "      <td>41.940100</td>\n",
       "      <td>-87.645500</td>\n",
       "      <td>41.940200</td>\n",
       "      <td>-87.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-06 16:17:07</td>\n",
       "      <td>2020-01-06 16:25:56</td>\n",
       "      <td>41.884600</td>\n",
       "      <td>-87.631900</td>\n",
       "      <td>41.891800</td>\n",
       "      <td>-87.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-30 08:37:16</td>\n",
       "      <td>2020-01-30 08:42:48</td>\n",
       "      <td>41.885600</td>\n",
       "      <td>-87.641800</td>\n",
       "      <td>41.889900</td>\n",
       "      <td>-87.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362513</th>\n",
       "      <td>2023-11-24 08:39:27</td>\n",
       "      <td>2023-11-24 08:47:03</td>\n",
       "      <td>41.936497</td>\n",
       "      <td>-87.647539</td>\n",
       "      <td>41.935775</td>\n",
       "      <td>-87.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362514</th>\n",
       "      <td>2023-11-06 09:07:20</td>\n",
       "      <td>2023-11-06 09:10:00</td>\n",
       "      <td>41.877726</td>\n",
       "      <td>-87.654787</td>\n",
       "      <td>41.877642</td>\n",
       "      <td>-87.649618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362515</th>\n",
       "      <td>2023-11-10 19:35:30</td>\n",
       "      <td>2023-11-10 19:44:28</td>\n",
       "      <td>41.943687</td>\n",
       "      <td>-87.648855</td>\n",
       "      <td>41.935775</td>\n",
       "      <td>-87.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362516</th>\n",
       "      <td>2023-11-27 09:11:23</td>\n",
       "      <td>2023-11-27 09:13:23</td>\n",
       "      <td>41.877726</td>\n",
       "      <td>-87.654787</td>\n",
       "      <td>41.877642</td>\n",
       "      <td>-87.649618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362517</th>\n",
       "      <td>2023-11-20 16:16:03</td>\n",
       "      <td>2023-11-20 16:17:43</td>\n",
       "      <td>41.877869</td>\n",
       "      <td>-87.654898</td>\n",
       "      <td>41.877642</td>\n",
       "      <td>-87.649618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20300267 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time           stop_time  start_latitude  \\\n",
       "0      2020-01-21 20:06:59 2020-01-21 20:14:30       41.966500   \n",
       "1      2020-01-30 14:22:39 2020-01-30 14:26:22       41.961600   \n",
       "2      2020-01-09 19:29:26 2020-01-09 19:32:17       41.940100   \n",
       "3      2020-01-06 16:17:07 2020-01-06 16:25:56       41.884600   \n",
       "4      2020-01-30 08:37:16 2020-01-30 08:42:48       41.885600   \n",
       "...                    ...                 ...             ...   \n",
       "362513 2023-11-24 08:39:27 2023-11-24 08:47:03       41.936497   \n",
       "362514 2023-11-06 09:07:20 2023-11-06 09:10:00       41.877726   \n",
       "362515 2023-11-10 19:35:30 2023-11-10 19:44:28       41.943687   \n",
       "362516 2023-11-27 09:11:23 2023-11-27 09:13:23       41.877726   \n",
       "362517 2023-11-20 16:16:03 2023-11-20 16:17:43       41.877869   \n",
       "\n",
       "        start_longitude  stop_latitude  stop_longitude  \n",
       "0            -87.688400      41.967100      -87.667400  \n",
       "1            -87.666000      41.954200      -87.664400  \n",
       "2            -87.645500      41.940200      -87.653000  \n",
       "3            -87.631900      41.891800      -87.620600  \n",
       "4            -87.641800      41.889900      -87.634300  \n",
       "...                 ...            ...             ...  \n",
       "362513       -87.647539      41.935775      -87.663600  \n",
       "362514       -87.654787      41.877642      -87.649618  \n",
       "362515       -87.648855      41.935775      -87.663600  \n",
       "362516       -87.654787      41.877642      -87.649618  \n",
       "362517       -87.654898      41.877642      -87.649618  \n",
       "\n",
       "[20300267 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_from_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T08:14:53.836775980Z",
     "start_time": "2023-12-23T08:14:53.662413649Z"
    }
   },
   "outputs": [],
   "source": [
    "trips_from_2020.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T08:14:53.810686409Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([trips_pre_2020, trips_from_2020], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_parquet(path = CLEANED_DATA/\"final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>start_latitude</th>\n",
       "      <th>start_longitude</th>\n",
       "      <th>stop_latitude</th>\n",
       "      <th>stop_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-06-30 23:57:00</td>\n",
       "      <td>2014-07-01 00:07:00</td>\n",
       "      <td>41.939304</td>\n",
       "      <td>-87.668278</td>\n",
       "      <td>41.945514</td>\n",
       "      <td>-87.646477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-06-30 23:56:00</td>\n",
       "      <td>2014-07-01 00:00:00</td>\n",
       "      <td>41.864819</td>\n",
       "      <td>-87.647128</td>\n",
       "      <td>41.869388</td>\n",
       "      <td>-87.655475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-06-30 23:33:00</td>\n",
       "      <td>2014-06-30 23:35:00</td>\n",
       "      <td>41.921687</td>\n",
       "      <td>-87.653714</td>\n",
       "      <td>41.919936</td>\n",
       "      <td>-87.648830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-06-30 23:26:00</td>\n",
       "      <td>2014-07-01 00:24:00</td>\n",
       "      <td>41.877702</td>\n",
       "      <td>-87.649654</td>\n",
       "      <td>49.318630</td>\n",
       "      <td>11.131904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-06-30 23:16:00</td>\n",
       "      <td>2014-06-30 23:26:00</td>\n",
       "      <td>41.872165</td>\n",
       "      <td>-87.661434</td>\n",
       "      <td>41.877702</td>\n",
       "      <td>-87.649654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362513</th>\n",
       "      <td>2023-11-24 08:39:27</td>\n",
       "      <td>2023-11-24 08:47:03</td>\n",
       "      <td>41.936497</td>\n",
       "      <td>-87.647539</td>\n",
       "      <td>41.935775</td>\n",
       "      <td>-87.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362514</th>\n",
       "      <td>2023-11-06 09:07:20</td>\n",
       "      <td>2023-11-06 09:10:00</td>\n",
       "      <td>41.877726</td>\n",
       "      <td>-87.654787</td>\n",
       "      <td>41.877642</td>\n",
       "      <td>-87.649618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362515</th>\n",
       "      <td>2023-11-10 19:35:30</td>\n",
       "      <td>2023-11-10 19:44:28</td>\n",
       "      <td>41.943687</td>\n",
       "      <td>-87.648855</td>\n",
       "      <td>41.935775</td>\n",
       "      <td>-87.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362516</th>\n",
       "      <td>2023-11-27 09:11:23</td>\n",
       "      <td>2023-11-27 09:13:23</td>\n",
       "      <td>41.877726</td>\n",
       "      <td>-87.654787</td>\n",
       "      <td>41.877642</td>\n",
       "      <td>-87.649618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362517</th>\n",
       "      <td>2023-11-20 16:16:03</td>\n",
       "      <td>2023-11-20 16:17:43</td>\n",
       "      <td>41.877869</td>\n",
       "      <td>-87.654898</td>\n",
       "      <td>41.877642</td>\n",
       "      <td>-87.649618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40359573 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                start_time           stop_time  start_latitude  \\\n",
       "0      2014-06-30 23:57:00 2014-07-01 00:07:00       41.939304   \n",
       "1      2014-06-30 23:56:00 2014-07-01 00:00:00       41.864819   \n",
       "2      2014-06-30 23:33:00 2014-06-30 23:35:00       41.921687   \n",
       "3      2014-06-30 23:26:00 2014-07-01 00:24:00       41.877702   \n",
       "4      2014-06-30 23:16:00 2014-06-30 23:26:00       41.872165   \n",
       "...                    ...                 ...             ...   \n",
       "362513 2023-11-24 08:39:27 2023-11-24 08:47:03       41.936497   \n",
       "362514 2023-11-06 09:07:20 2023-11-06 09:10:00       41.877726   \n",
       "362515 2023-11-10 19:35:30 2023-11-10 19:44:28       41.943687   \n",
       "362516 2023-11-27 09:11:23 2023-11-27 09:13:23       41.877726   \n",
       "362517 2023-11-20 16:16:03 2023-11-20 16:17:43       41.877869   \n",
       "\n",
       "        start_longitude  stop_latitude  stop_longitude  \n",
       "0            -87.668278      41.945514      -87.646477  \n",
       "1            -87.647128      41.869388      -87.655475  \n",
       "2            -87.653714      41.919936      -87.648830  \n",
       "3            -87.649654      49.318630       11.131904  \n",
       "4            -87.661434      41.877702      -87.649654  \n",
       "...                 ...            ...             ...  \n",
       "362513       -87.647539      41.935775      -87.663600  \n",
       "362514       -87.654787      41.877642      -87.649618  \n",
       "362515       -87.648855      41.935775      -87.663600  \n",
       "362516       -87.654787      41.877642      -87.649618  \n",
       "362517       -87.654898      41.877642      -87.649618  \n",
       "\n",
       "[40359573 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-Frx8CZzu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
